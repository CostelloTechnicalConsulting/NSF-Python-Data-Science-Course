{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTHORITATIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all NSF-Awards data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile  \n",
    "import io\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "data_dir = '../data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data\n",
    "Download all zip files for awards from 2010 thru 2025, extract all json, load all json into a big dataframe, then subdivide into dataframes for awards, PIs, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading awards data for 2010...\n",
      "zip file for 2010 downloaded\n",
      "zip file for 2010 unzipped\n",
      "Downloading awards data for 2011...\n",
      "zip file for 2011 downloaded\n",
      "zip file for 2011 unzipped\n",
      "Downloading awards data for 2012...\n",
      "zip file for 2012 downloaded\n",
      "zip file for 2012 unzipped\n",
      "Downloading awards data for 2013...\n",
      "zip file for 2013 downloaded\n",
      "zip file for 2013 unzipped\n",
      "Downloading awards data for 2014...\n",
      "zip file for 2014 downloaded\n",
      "zip file for 2014 unzipped\n",
      "Downloading awards data for 2015...\n",
      "zip file for 2015 downloaded\n",
      "zip file for 2015 unzipped\n",
      "Downloading awards data for 2016...\n",
      "zip file for 2016 downloaded\n",
      "zip file for 2016 unzipped\n",
      "Downloading awards data for 2017...\n",
      "zip file for 2017 downloaded\n",
      "zip file for 2017 unzipped\n",
      "Downloading awards data for 2018...\n",
      "zip file for 2018 downloaded\n",
      "zip file for 2018 unzipped\n",
      "Downloading awards data for 2019...\n",
      "zip file for 2019 downloaded\n",
      "zip file for 2019 unzipped\n",
      "Downloading awards data for 2020...\n",
      "zip file for 2020 downloaded\n",
      "zip file for 2020 unzipped\n",
      "Downloading awards data for 2021...\n",
      "zip file for 2021 downloaded\n",
      "zip file for 2021 unzipped\n",
      "Downloading awards data for 2022...\n",
      "zip file for 2022 downloaded\n",
      "zip file for 2022 unzipped\n",
      "Downloading awards data for 2023...\n",
      "zip file for 2023 downloaded\n",
      "zip file for 2023 unzipped\n",
      "Downloading awards data for 2024...\n",
      "zip file for 2024 downloaded\n",
      "zip file for 2024 unzipped\n",
      "Downloading awards data for 2025...\n",
      "zip file for 2025 downloaded\n",
      "zip file for 2025 unzipped\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create directories for zip files and unzipped data if they don't exist\n",
    "for year in range(2010, 2026):\n",
    "    awards_url = f\"https://www.nsf.gov/awardsearch/download?DownloadFileName={year}&All=true&isJson=true\"\n",
    "    print(f\"Downloading awards data for {year}...\")\n",
    "    \n",
    "    # Download the zip file from the URL\n",
    "    response = requests.get(awards_url)\n",
    "    with open(f\"zipfiles/awards_{year}.zip\", \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    print(f\"zip file for {year} downloaded\")\n",
    "\n",
    "    # Unzip the file\n",
    "    with zipfile.ZipFile(f\"zipfiles/awards_{year}.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(f\"data/awards_json/awards_{year}\")\n",
    "\n",
    "    print(f\"zip file for {year} unzipped\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading JSON files into DataFrame...\n",
      "DataFrame created\n"
     ]
    }
   ],
   "source": [
    "# # Read all json files from the unzipped folder into a single DataFrame using the Pandas normalize_json function\n",
    "def read_json_files(folder_path):\n",
    "    dataframes = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            filename = os.path.join(root, file)\n",
    "            if filename.endswith(\".json\"):\n",
    "                with open(filename, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    df = pd.json_normalize(data)\n",
    "                    dataframes.append(df)\n",
    "    # Concatenate all DataFrames into a single DataFrame, resetting the index before returning\n",
    "    return pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "print(\"Reading JSON files into DataFrame...\")\n",
    "\n",
    "# Read the JSON files into a DataFrame\n",
    "awards_df = read_json_files(\"../data/awards_json/\")\n",
    "\n",
    "print(\"DataFrame created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update datatypes in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date columns to datetime format\n",
    "date_columns = ['awd_eff_date', 'awd_exp_date','awd_min_amd_letter_date', 'awd_max_amd_letter_date']\n",
    "for col in date_columns:\n",
    "    if col in awards_df.columns:\n",
    "        awards_df[col] = pd.to_datetime(awards_df[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and restore the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awards_df.to_pickle('../data/pkl_files/awards_data_2010_2025_big.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "awards_df = pd.read_pickle('../data/pkl_files/awards_data_2010_2025_big.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract child tables\n",
    "The pi, pgm_ele, pgm_ref, app_fund, and oblg_fy columns all contain nested data and will be broken out into separate data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pi table\n",
    "# Extract the 'pi' column from the awards_data DataFrame, which contains nested data.\n",
    "# Use the explode() method to transform each element of a list-like column into a separate row.\n",
    "# Use the apply() method with the pandas Series constructor to convert the exploded data into a DataFrame.\n",
    "# Dictionary keys will become column names, and values will become the corresponding values in the DataFrame.\n",
    "pi_df = awards_df['pi'].explode().apply(pd.Series) # 330,573 rows, 10 columns\n",
    "\n",
    "# Remove duplicates from the pi_df DataFrame based on the 'nsf_id' column.\n",
    "# Keep the first occurrence of each unique value in the 'nsf_id' column.\n",
    "# Reset the index of the DataFrame after removing duplicates.   \n",
    "# The reset_index() method is used to create a new index for the DataFrame.\n",
    "# The drop=True argument is used to avoid adding the old index as a new column in the DataFrame.\n",
    "\n",
    "pi_df = pi_df.drop_duplicates(subset=['nsf_id']).reset_index(drop=True) # 131,363 rows, 10 columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pgm_ele table\n",
    "# Same process as above, but for the 'pgm_ele' column.\n",
    "pgm_ele_df = awards_df['pgm_ele'].explode().apply(pd.Series) # 243,844 rows, 2 columns\n",
    "\n",
    "# Remove duplicates from the pgm_ele_df DataFrame based on the 'pgm_ele_code' column.\n",
    "\n",
    "pgm_ele_df = pgm_ele_df.drop_duplicates(subset=['pgm_ele_code']).reset_index(drop=True) # 1,190 rows, 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pgm_ref table\n",
    "# Same process as above, but for the 'pgm_ref' column.\n",
    "pgm_ref_df = awards_df['pgm_ref'].explode().apply(pd.Series) # 476,124 rows, 2 columns\n",
    "\n",
    "# Remove duplicates from the pgm_ref_df DataFrame based on the 'pgm_ref_code' column.\n",
    "pgm_ref_df = pgm_ref_df.drop_duplicates(subset=['pgm_ref_code']).reset_index(drop=True) # 1,509 rows, 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app_fund table\n",
    "# Same process as above, but for the 'app_fund' column.\n",
    "app_fund_df = awards_df['app_fund'].explode().apply(pd.Series) # 321,721 rows x 6 columns\n",
    "# Remove duplicates from the app_fund_df DataFrame based on the 'fund_code' column. \n",
    "\n",
    "app_fund_df = app_fund_df.drop_duplicates(subset=['fund_code']).reset_index(drop=True) # 213 rows x 6 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oblg_fy table\n",
    "# Same process as above, but for the 'oblg_fy' column.\n",
    "oblg_fy_df = awards_df['oblg_fy'].explode().apply(pd.Series) # 292,009 rows, 2 columns\n",
    "\n",
    "# Reset the index, retaining the original index as a new column.\n",
    "oblg_fy_df = oblg_fy_df.reset_index()\n",
    "\n",
    "# Rename the 'index' column to 'awd_index'.\n",
    "oblg_fy_df = oblg_fy_df.rename(columns={'index': 'awd_index'}) # 292,009 rows, 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrames to pickle files\n",
    "pi_df.to_pickle('../data/pkl_files/pi_df.pkl')\n",
    "pgm_ele_df.to_pickle('../data/pkl_files/pgm_ele_df.pkl')\n",
    "pgm_ref_df.to_pickle('../data/pkl_files/pgm_ref_df.pkl')\n",
    "app_fund_df.to_pickle('../data/pkl_files/app_fund_df.pkl')\n",
    "oblg_fy_df.to_pickle('../data/pkl_files/oblg_fy_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the pickle files back into DataFrames\n",
    "pi_df = pd.read_pickle('../data/pkl_files/pi_df.pkl')\n",
    "pgm_ele_df = pd.read_pickle('../data/pkl_files/pgm_ele_df.pkl')\n",
    "pgm_ref_df = pd.read_pickle('../data/pkl_files/pgm_ref_df.pkl')\n",
    "app_fund_df = pd.read_pickle('../data/pkl_files/app_fund_df.pkl')\n",
    "oblg_fy_df = pd.read_pickle('../data/pkl_files/oblg_fy_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create bridge tables to join parent awards table to child tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bridge table to connect the pi_df and awards_data DataFrames\n",
    "\n",
    "awd_pi_list = []\n",
    "for row in awards_df.iterrows():\n",
    "    # Get the index of the current row\n",
    "    index = row[0]\n",
    "\n",
    "    # Get the awd_id of the current row\n",
    "    awd_id = row[1]['awd_id']\n",
    "    \n",
    "    # Get the list of PI IDs for the current award\n",
    "    pi_ids = row[1]['pi']\n",
    "    # Check if pi_ids is not None and is a list\n",
    "    if pi_ids is not None and isinstance(pi_ids, list):\n",
    "        # If pi_ids is a list, extract the 'nsf_id' from each element\n",
    "        pi_ids = [{'nsf_id': pi['nsf_id']} for pi in pi_ids]\n",
    "    else:\n",
    "        # If pi_ids is not a list, create an empty list\n",
    "        pi_ids = []\n",
    "    for pi_id in pi_ids:\n",
    "        # Create a dictionary for the bridge table\n",
    "        awd_pi_dict = {\n",
    "            'awd_index': index,\n",
    "            'awd_id': awd_id,\n",
    "            'nsf_id': pi_id['nsf_id']\n",
    "        }\n",
    "        # Append the dictionary to the list\n",
    "        awd_pi_list.append(awd_pi_dict)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "awd_pi_df = pd.DataFrame(awd_pi_list) # 330,573 rows, 3 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bridge table to connect the pgm_ele_df and awards_data DataFrames\n",
    "awd_pgm_ele_list = []\n",
    "\n",
    "for row in awards_df.iterrows():\n",
    "    # Get the index of the current row\n",
    "    index = row[0]\n",
    "\n",
    "    # Get the awd_id of the current row\n",
    "    awd_id = row[1]['awd_id']\n",
    "    \n",
    "    # Get the list of program elements for the current award\n",
    "    pgm_elems = row[1]['pgm_ele']\n",
    "    # Check if pgm_elems is not None and is a list\n",
    "    if pgm_elems is not None and isinstance(pgm_elems, list):\n",
    "        # If pgm_elems is a list, extract the 'pgm_ele_code' from each element\n",
    "        pgm_elems = [{'pgm_ele_code': pgm['pgm_ele_code']} for pgm in pgm_elems]\n",
    "    else:\n",
    "        # If pgm_elems is not a list, create an empty list\n",
    "        pgm_elems = []\n",
    "    for pgm_elem in pgm_elems:\n",
    "        # Create a dictionary for the bridge table\n",
    "        awd_pgm_elem_dict = {\n",
    "            'awd_index': index,\n",
    "            'awd_id': awd_id,\n",
    "            'pgm_ele_code': pgm_elem['pgm_ele_code']\n",
    "        }\n",
    "        # Append the dictionary to the list\n",
    "        awd_pgm_ele_list.append(awd_pgm_elem_dict)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "awd_pgm_ele_df = pd.DataFrame(awd_pgm_ele_list) # 243,152 rows, 3 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bridge table to connect the awards_data and pgm_ref_df DataFrames\n",
    "awd_pgm_ref_list = []\n",
    "\n",
    "for row in awards_df.iterrows():\n",
    "    # Get the index of the current row\n",
    "    index = row[0]\n",
    "\n",
    "    # Get the awd_id of the current row\n",
    "    awd_id = row[1]['awd_id']\n",
    "    \n",
    "    # Get the list of program references for the current award\n",
    "    pgm_refs = row[1]['pgm_ref']\n",
    "    # Check if pgm_refs is not None and is a list\n",
    "    if pgm_refs is not None and isinstance(pgm_refs, list):\n",
    "        # If pgm_refs is a list, extract the 'pgm_ref_code' from each element\n",
    "        pgm_refs = [{'pgm_ref_code': pgm['pgm_ref_code']} for pgm in pgm_refs]\n",
    "    else:\n",
    "        # If pgm_refs is not a list, create an empty list\n",
    "        pgm_refs = []\n",
    "    for pgm_ref in pgm_refs:\n",
    "        # Create a dictionary for the bridge table\n",
    "        awd_pgm_ref_dict = {\n",
    "            'awd_index': index,\n",
    "            'awd_id': awd_id,\n",
    "            'pgm_ref_code': pgm_ref['pgm_ref_code']\n",
    "        }\n",
    "        # Append the dictionary to the list\n",
    "        awd_pgm_ref_list.append(awd_pgm_ref_dict)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "awd_pgm_ref_df = pd.DataFrame(awd_pgm_ref_list) # 451,944 rows, 3 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the bridge tables to pickle files\n",
    "awd_pi_df.to_pickle('../data/pkl_files/awd_pi_df.pkl')\n",
    "awd_pgm_ele_df.to_pickle('../data/pkl_files/awd_pgm_ele_df.pkl')\n",
    "awd_pgm_ref_df.to_pickle('../data/pkl_files/awd_pgm_ref_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the bridge tables back into DataFrames\n",
    "awd_pi_df = pd.read_pickle('../data/pkl_files/awd_pi_df.pkl')\n",
    "awd_pgm_ele_df = pd.read_pickle('../data/pkl_files/awd_pgm_ele_df.pkl')\n",
    "awd_pgm_ref_df = pd.read_pickle('../data/pkl_files/awd_pgm_ref_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove unnecessary columns from awards data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['awd_id', 'agcy_id', 'tran_type', 'awd_istr_txt', 'awd_titl_txt',\n",
       "       'cfda_num', 'org_code', 'po_phone', 'po_email', 'po_sign_block_name',\n",
       "       'awd_eff_date', 'awd_exp_date', 'tot_intn_awd_amt', 'awd_amount',\n",
       "       'awd_min_amd_letter_date', 'awd_max_amd_letter_date',\n",
       "       'awd_abstract_narration', 'awd_arra_amount', 'dir_abbr',\n",
       "       'org_dir_long_name', 'div_abbr', 'org_div_long_name', 'awd_agcy_code',\n",
       "       'fund_agcy_code', 'pi', 'pgm_ele', 'pgm_ref', 'app_fund', 'oblg_fy',\n",
       "       'inst.inst_name', 'inst.inst_street_address',\n",
       "       'inst.inst_street_address_2', 'inst.inst_city_name',\n",
       "       'inst.inst_state_code', 'inst.inst_state_name', 'inst.inst_phone_num',\n",
       "       'inst.inst_zip_code', 'inst.inst_country_name', 'inst.cong_dist_code',\n",
       "       'inst.st_cong_dist_code', 'inst.org_lgl_bus_name',\n",
       "       'inst.org_prnt_uei_num', 'inst.org_uei_num', 'perf_inst.perf_inst_name',\n",
       "       'perf_inst.perf_str_addr', 'perf_inst.perf_city_name',\n",
       "       'perf_inst.perf_st_code', 'perf_inst.perf_st_name',\n",
       "       'perf_inst.perf_zip_code', 'perf_inst.perf_ctry_code',\n",
       "       'perf_inst.perf_cong_dist', 'perf_inst.perf_st_cong_dist',\n",
       "       'perf_inst.perf_ctry_name', 'perf_inst.perf_ctry_flag', 'por.por_cntn',\n",
       "       'por.por_txt_cntn', 'por'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awards_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before\n",
    "# awards_df.info(memory_usage='deep') # memory usage: 2.1 GB\n",
    "\n",
    "awards_df.drop(columns=['awd_abstract_narration','pi', 'pgm_ele', 'pgm_ref', 'app_fund', 'oblg_fy', 'por.por_cntn',\n",
    "       'por.por_txt_cntn', 'por'], inplace=True)\n",
    "\n",
    "# after\n",
    "# awards_df.info(memory_usage='deep') # memory usage: 559.8 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the cleaned awards_df DataFrame to a pickle file\n",
    "awards_df.to_pickle('../data/pkl_files/awards_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the awards_df DataFrame back from the pickle file\n",
    "awards_df = pd.read_pickle('../data/pkl_files/awards_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make two partially summarized dataframes\n",
    "By year and division, by year and directorate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize awards_df, grouping by year of the awd_eff_date column and div_abbr, and including the count of awards and the sum of the award amount\n",
    "awards_summary_divyr_df = awards_df.groupby([awards_df['awd_eff_date'].dt.year, 'div_abbr']).agg(\n",
    "    awd_count=('awd_id', 'count'),\n",
    "    sum_awd_amount=('awd_amount', 'sum')\n",
    ").reset_index()\n",
    "# Rename the columns for clarity\n",
    "awards_summary_divyr_df.rename(columns={'awd_eff_date': 'awd_year'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize awards_df, grouping by year of the awd_eff_date column and dir_abbr, and including the count of awards and the sum of the award amount\n",
    "awards_summary_diryr_df = awards_df.groupby([awards_df['awd_eff_date'].dt.year, 'dir_abbr']).agg(\n",
    "    awd_count=('awd_id', 'count'),\n",
    "    sum_awd_amount=('awd_amount', 'sum')\n",
    ").reset_index()\n",
    "# Rename the columns for clarity\n",
    "awards_summary_diryr_df.rename(columns={'awd_eff_date': 'awd_year'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the summary DataFrames to pickle files\n",
    "awards_summary_diryr_df.to_pickle('../data/pkl_files/awards_summary_diryr_df.pkl')\n",
    "awards_summary_divyr_df.to_pickle('../data/pkl_files/awards_summary_divyr_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the summary DataFrames back from the pickle files\n",
    "awards_summary_diryr_df = pd.read_pickle('../data/pkl_files/awards_summary_diryr_df.pkl')\n",
    "awards_summary_divyr_df = pd.read_pickle('../data/pkl_files/awards_summary_divyr_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make an enriched version of awards_df with an awd_type column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join awards_df with awd_pgm_ref_df on the 'awd_id' column\n",
    "awards_with_type_df = pd.merge(awards_df, awd_pgm_ref_df, on='awd_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an awd_type column based on pgm_ref_code\n",
    "rc_at_map = {\n",
    "    '1045' : 'CAREER',\n",
    "    '9250' : 'REU Site',\n",
    "    '9229' : 'RUI'\n",
    "}\n",
    "\n",
    "awards_with_type_df['awd_type'] = awards_with_type_df['pgm_ref_code'].map(rc_at_map)\n",
    "awards_with_type_df = awards_with_type_df.query('awd_type == \"CAREER\" or awd_type == \"REU Site\" or awd_type == \"RUI\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to pickle file\n",
    "awards_with_type_df.to_pickle(f'{data_dir}/pkl_files/awards_with_type_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the awards_with_type_df DataFrame back from the pickle file\n",
    "awards_with_type_df = pd.read_pickle(f'{data_dir}/pkl_files/awards_with_type_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove everything we don't need from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del awards_data\n",
    "if 'awd_pgm_ele_list' in globals(): del awd_pgm_ele_list\n",
    "if 'awd_pgm_ref_list' in globals(): del awd_pgm_ref_list\n",
    "if 'awd_pi_list' in globals(): del awd_pi_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for root, dirs, files in os.walk(\"data/awards_json/\"):\n",
    "#     for file in files:\n",
    "#         print(os.path.join(root, file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
